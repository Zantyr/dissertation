\documentclass[12pt,a4paper,oneside]{report} %default 10pt
\usepackage[utf8]{inputenc}
\usepackage{geometry} % to change the page dimensions
\usepackage{booktabs} % for much better looking tables
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{float}
\usepackage{amsfonts}
\usepackage{ifpdf}
\usepackage{indentfirst}
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[polish]{babel}
\usepackage{caption}
\usepackage{mathtools}

\geometry{a4paper}
\geometry{margin=1in}
\pagestyle{plain}
\renewcommand{\headrulewidth}{0pt}
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}
\setlength\parindent{24pt}

\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
    language=Python,
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\small\ttfamily},
    numbers=none,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=3
}

\title{System doradztwa zawodowego z wykorzystaniem metod sztucznej inteligencji}
\author{Paweł Tomasik}

\begin{document}
\maketitle
\begin{abstract}
Przepisać na Pythona 3.5
Wstęp do pracy oto jedna strona.\par
Wstęp do zagadnienia - kilka stron \par
Wstęp do drugiego zagadnienia - kilka stron \par
Opis implementacji - najwięcej \par
Wyniki - dużo stron\par
Analiza wyników i podsumowanie \par
\title{Słowa kluczowe}
Python
\end{abstract}
\renewcommand{\abstractname}{Strzeszczenie}
\begin{abstract}
Przepisać na Pythona 3.5
\end{abstract}

\tableofcontents

\section{Temat pracy}

\section{Klasyfikatory i klasteryzacja - rodzaje algorytmów i porównanie}
\subsection{Czym jest problem klasyfikacji i klasteryzacji}

Zarówno problem klasyfikacji i klasteryzacji definiuje się w podobny sposób: istotą problemu jest przydzielenie każdego elementu ze skończonego zbioru \emph{obiektów} do jednej lub kilku grup zwanych \emph{klasami}. Różnica między dwoma pojęciami leży w sposobie uczenia algorytmu: klasteryzacja wykorzystuje sprzężenie zwrotne co do poprawnej klasyfikacji - jest więc przykładem algorytmu uczenia maszynowego \emph{nadzorowanego}. Algorytmy klasteryzacyjne natomiast próbują same dokonać wytworzenia wzorca klas - należą do algorytmów uczenia \emph{nienadzorowanego}.\par

Aby podział na klasy był satysfakcjonujący, musi spełniać dwa kryteria. Po pierwsze, elementy wewnątrz klasy muszą być mozliwie do siebie podobne. Po drugie, elementy z różnych klas muszą różnić się od siebie o ile jest to tylko możliwe.\par

Aby możliwe było opisanie problemu, potrzebna jest formalne zdefiniowanie zarówno cech, jak i podobieństwa. W praktyce cechy można definiować dwojako: jako wartości binarne lub rzeczywiste (znormalzowane do jedności, lub nie). W ten sposób opis danego obiektu przyjmuje postać wektora cech (ang. \emph{features}). W takiej reprezentacji miarą podobieństwa, lub bardziej niepodobieństwa, jest odległość między tymi dwoma wektorami. Formalnie podobieństwo $s$ definiuje się jako:\par
    \begin{equation}
    tu-wzor
    \end{equation}
Ważnym pojęciem do zmierzenia podobieństwa między dwoma obiektami jest pojęcie \emph{metryki}. Metryka jest funkcją, która przyjmując dwa wektory zwraca odległość między nimi. O ile pojęcie odległości pojmowane intuicyjnie jest jednoznaczne, jednak dla celów rozróżnienia obiektów, które są wektorami w przestrzeni o bardzo wielu wymiarach, odpowiednio dobrana metryka pozwala na lepsze dopasowanie modelu klasy do rzeczywistego rozkładu cech wśród jej członków.\par

Najważniejsze dwie metryki to metryka euklidesowa, oraz metryka Manhattan. Są one rozszerzeniem ogólnej metryki Minkowskiego, podanej we wzorze (?). Szczególnym przypadkiem metryki typu Manhattan jest metryka Hamminga, która dla ciągów binarnych jest analogiem sumą bitów jedynkowych funkcji XOR. \par

Drugim elementem, poza metryką, definiującym kształt klasy jest \emph{norma}. Norma ma najczęściej postać macierzy $n \times n$ współczynników, gdzie $n$ to długość wektora cech, w której znajdują się współczynniki przekształcające wektor w taki sposób, aby zrównoważyć wpływ różnego rozrzutu cech na klasyfikację, jak i uwzględnić zależności między cechami. \par

Uzbrojeni w definicję podobieństwa i sposób jego obliczenia, jesteśmy w stanie w miarę dobrze ocenić poprawność działania algorytmów. Ważnym elementem różnych rozwiązań jest też sposób klasyfikacji: dzieli się ją na \emph{ostrą}, \emph{rozmytą} i \emph{posybilistyczną}. Klasyfikacja ostra przyporządkowuje obiekt do jednej i tylko jednej klasy. Klasyfikacja rozmyta wiąże obiekt z róznymi klasami z różną siłą, przy czym suma współczynników określających przynależność dla danego obiektu jest zawsze równa jedności. Klasifykacja posybilistyczna różni się od poprzedniej zniesieniem ograniczenia sumy.\par

\subsection{State-of-the-art metod sztucznej inteligencji}
\subsection{Binarny klasyfikator Bernoulliego}
\subsection{Sieć neuronowa}
\section{Badania zawodowe i psychologiczne}
\subsection{Obecne systemy klasyfikacji psychologicznej}
\subsection{Kryteria wyboru metod}
\subsection{Model Hollanda (ten z sześcioma opcjami)}
\subsection{Model MBTI (z szesnastoma klasami)}
16FP Questionnaire, enneagramme, big five personality traits
\subsection{Lista badanych zawodów}
\section{Wybór cech wejściowych}
\subsection{Problem ograniczenia ilości pytań}
\subsection{Okrojenie modeli psychologicznych}
\subsection{Dostosowanie pytań dodatkowych (data mining)}
\section{Implementacja i zbieranie danych}
\subsection{Moduł inteligentny}
\subsection{Moduł respondenta}
\subsection{Moduł uczenia wsadowego}
\subsection{Moduł prezentacji danych}
\subsection{Moduł kreacji testów}
\subsection{Aplikacja systemu do zadanego problemu}
\subsection{Schemat}
\section{Wyniki}
\subsection{Porównanie trafności klasyfikatorów}
\subsection{Wnioski wyciągnięte z badań}
\section{Kod źródłowy}
\subsection{Wsadowy skrypt instalacyjny (Unix)}
\subsection{Kod źródłowy modułu inteligentnego}
\lstinputlisting[breaklines]{src/main.py}

\begin{thebibliography}{9}
\bibitem{cichosz} Paweł Cichosz, \emph{Systemy uczące się}
\bibitem{flasinski} Mariusz Flasiński, \emph{Wstęp do sztucznej inteligencji}
\bibitem{rutkowski} Leszek Rutkowski, \emph{Metody i techniki sztucznej inteligencji}, Warszawa 2005, PWN, rozdziały 6, 8, 10
\bibitem{szczepaniak} Piotr S. Szczepaniak \emph{Obliczenia inteligentne, szybkie przekształcenia i klasyfikatory}, Warszawa 2004, Akademicka Oficyna Wydawnicza EXIT
\bibitem{wojcik} Waldemar Wójcik et alii, \emph{Sztuczna inteligencja i metody optymalizacji - od teorii do praktyki}, Lublin 2008, Polskie Towarzystwo Informatyczne
\bibitem{parol} Mirosław Parol, Paweł Piotrowski et alii, \emph{Sztuczna inteligencja w praktyce - Laboratorium}, ćwiczenie 3
\bibitem{malina} Witold Malina, Maciej Smiatacz, \emph{Rozpoznawanie obrazów}, Warszawa 2010, akademicka Oficyna Wydawnicza EXIT
\bibitem{podolak} Igor T. Podolak, \emph{Klasyfikator Hierarchiczny z nakładającymi się grupami klas}, Kraków 2012, Wydawnictwo Uniwersytetu Jagiellońskiego
\bibitem{hastie} Trevor Hastie, Robert Tibshirani, Jerome Friedman, \emph{The Elements of Statistical Learning}, Springer, druga edycja
\bibitem{pythonwiki} \emph{https://wiki.python.org/moin/PythonForArtificialIntelligence}
\bibitem{stanford} \emph{http://cs.stanford.edu/people/eroberts/courses/soco/projects/2000-01/neural-networks/Sources/index.html}
\end{thebibliography}
\end{document}
